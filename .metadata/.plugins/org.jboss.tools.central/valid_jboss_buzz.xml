<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Managing Python dependencies with the Thoth JupyterLab extension</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lFgYvP4zKGE/" /><category term="Machine Learning" /><category term="Microservices" /><category term="Open source" /><category term="Python" /><category term="Jupyter notebook" /><category term="JupyterLab" /><category term="Project Thoth" /><category term="Python dependencies" /><author><name>Francesco Murdaca</name></author><id>https://developers.redhat.com/blog/?p=855857</id><updated>2021-03-19T07:00:12Z</updated><published>2021-03-19T07:00:12Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/jupyterlab/jupyterlab"&gt;JupyterLab&lt;/a&gt; is a flexible and powerful tool for working with Jupyter notebooks. Its interactive user interface (UI) lets you use terminals, text editors, file browsers, and other components alongside your Jupyter notebook. JupyterLab 3.0 was released in January 2021.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; develops &lt;a target="_blank" rel="nofollow" href="/topics/open-source"&gt;open source&lt;/a&gt; tools that enhance the day-to-day lives of developers and data scientists. Thoth uses machine-generated knowledge to boost your applications&amp;#8217; performance, security, and quality through reinforcement learning with &lt;a target="_blank" rel="nofollow" href="/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt;. (&lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=WEJ65Rvj3lc&amp;#38;t=1s"&gt;Watch this video&lt;/a&gt; to learn more about resolving dependencies with reinforcement learning.)&lt;/p&gt; &lt;p&gt;This machine learning approach is implemented in &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth adviser&lt;/a&gt;, a recommendation engine for Python applications. &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/integration.rst"&gt;Thoth integrations&lt;/a&gt; use this knowledge to provide software stack recommendations based on &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos#using-custom-configuration-file-template"&gt;user inputs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article introduces you to &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt;, a JupyterLab extension for managing and optimizing Python dependencies in your Jupyter notebooks. As you will learn, using the &lt;code&gt;jupyterlab-requirements&lt;/code&gt; extension is a smart and easy way to ensure that your code and experiments are always reproducible.&lt;/p&gt; &lt;h2&gt;Making application dependencies reproducible&lt;/h2&gt; &lt;p&gt;When creating code or conducting experiments, reproducibility is an important requirement. Ensuring that others can rerun experiments in the same environment the creator used is critical, especially when developing machine learning applications.&lt;/p&gt; &lt;p&gt;Let’s consider one of the first steps for developing an application: specifying dependencies. For example, your project might depend on &lt;a target="_blank" rel="nofollow" href="https://pandas.pydata.org/docs/getting_started/index.html"&gt;pandas&lt;/a&gt; for data exploration and manipulation or &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/tensorflow/"&gt;TensorFlow&lt;/a&gt; for training a model.&lt;/p&gt; &lt;p&gt;One approach to this task is to run a command in the notebook cell to install the dependencies directly on the host, as shown in Figure 1. This way, the next user can run the same cell and install similar packages.&lt;/p&gt; &lt;div id="attachment_855977" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png"&gt;&lt;img aria-describedby="caption-attachment-855977" class="wp-image-855977" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png" alt="A user installing dependencies in the notebook cell using the pip install command." width="640" height="102" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16.png 848w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16-300x48.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-10-28-11-18-16-768x122.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-855977" class="wp-caption-text"&gt;Figure 1: Installing dependencies directly on the host using the &lt;code&gt;pip install&lt;/code&gt; command.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Another potential strategy is to provide a &lt;code&gt;requirements.txt&lt;/code&gt; file that lists all of the dependencies so that someone else can install them before starting the notebook. Figure 2 shows an example.&lt;/p&gt; &lt;div id="attachment_855987" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png"&gt;&lt;img aria-describedby="caption-attachment-855987" class="wp-image-855987" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png" alt="A sample list of software dependencies in a text file." width="640" height="102" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21.png 1131w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-300x48.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-768x123.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2020-11-03-08-31-21-1024x164.png 1024w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-855987" class="wp-caption-text"&gt;Figure 2: A sample list of dependencies.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Do you see any issues with these two approaches to specifying dependencies?&lt;/p&gt; &lt;p&gt;Neither one supports reproducibility!&lt;/p&gt; &lt;p&gt;In the first scenario, let&amp;#8217;s say another user tried to rerun the same cell sometime after a new version of the library was released. They might experience different behavior from the initial notebook output.&lt;/p&gt; &lt;p&gt;The same issue can arise with the &lt;code&gt;requirements.txt&lt;/code&gt; file, only with the package names. Even if you stated the direct dependencies with the exact version number, each of those dependencies might depend on other so-called &lt;em&gt;transitive dependencies&lt;/em&gt; that are also installed.&lt;/p&gt; &lt;p&gt;To guarantee reproducibility, you must account for all dependencies with specific version numbers for direct and transitive dependencies, including all hashes used to verify the provenance of the packages for security reasons (check these &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/docs/developers/adviser/provenance_checks.html"&gt;docs&lt;/a&gt; to learn more about security in software stacks). To be even more precise, the Python version, operating system, and hardware all influence the code’s behavior. You should share all of this information so other users can experience the same behavior and obtain similar results.&lt;/p&gt; &lt;p&gt;Project Thoth aims to help you specify direct and transitive dependencies so that your applications are always reproducible and you can focus on more pressing challenges.&lt;/p&gt; &lt;h2&gt;Dependency management with jupyterlab-requirements&lt;/h2&gt; &lt;p&gt;The Thoth team has introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt;, a JupyterLab extension for dependency management that is currently focused on the &lt;a target="_blank" rel="nofollow" href="/blog/category/python/"&gt;Python&lt;/a&gt; ecosystem. This extension lets you manage your project&amp;#8217;s dependencies directly from a Jupyter notebook, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_856057" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg"&gt;&lt;img aria-describedby="caption-attachment-856057" class="wp-image-856057" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg" alt="Screenshot of the jupyterlab-requirements extension with the Managed Dependencies menu item highlighted." width="640" height="202" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements.jpg 1950w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-300x95.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-768x243.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/jupyterlab-requirements-1024x323.jpg 1024w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856057" class="wp-caption-text"&gt;Figure 3: Managing dependencies in JupyterLab.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;When you click &lt;strong&gt;Manage Dependencies&lt;/strong&gt;, you will see the dialog box shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_856077" style="width: 523px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-856077" class="wp-image-856077 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00.png" alt="A dialog box stating ‘No dependencies found! Click button above to add new packages.’" width="513" height="243" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00.png 513w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-00-300x142.png 300w" sizes="(max-width: 513px) 100vw, 513px" /&gt;&lt;p id="caption-attachment-856077" class="wp-caption-text"&gt;Figure 4: A new notebook with no dependencies identified.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Initially, the extension will not identify any dependencies when you start a new notebook; it checks the notebook metadata to detect them. You can add your packages by clicking the button with the plus-sign (+) icon, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_856087" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png"&gt;&lt;img aria-describedby="caption-attachment-856087" class="wp-image-856087" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png" alt="Screenshot of the Manage Dependencies screen with the option to add new package dependencies." width="640" height="291" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48.png 794w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48-300x136.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-31-48-768x349.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856087" class="wp-caption-text"&gt;Figure 5: Adding new packages with the jupyterlab-requirements extension.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After saving, an &lt;strong&gt;Install&lt;/strong&gt; button will appear. You can check the package names and versions before installing the dependencies, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_856097" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png"&gt;&lt;img aria-describedby="caption-attachment-856097" class="wp-image-856097" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png" alt="Screenshot of the Manage Dependencies screen with the Install button and sample packages added." width="640" height="427" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42.png 841w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42-300x200.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-46-42-768x512.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856097" class="wp-caption-text"&gt;Figure 6: The Manage Dependencies screen with sample packages added and ready to install.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After clicking Install, you will see the screen shown in Figure 7.&lt;/p&gt; &lt;div id="attachment_856107" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png"&gt;&lt;img aria-describedby="caption-attachment-856107" class="wp-image-856107" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png" alt="Screenshot of the Manage Dependencies screen with requirements locked, saved, and installed." width="640" height="361" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12.png 788w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12-300x169.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/Screenshot-from-2021-01-11-18-53-12-768x433.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-856107" class="wp-caption-text"&gt;Figure 7: The packages are locked, saved, and installed in the notebook metadata.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;All dependencies—both direct and transitive—will be locked, saved in the notebook metadata, and installed. What’s more, the extension automatically creates and sets the kernel for your notebook. No human intervention is necessary, and you are ready to work on your project.&lt;/p&gt; &lt;h2&gt;Managing dependencies in an existing notebook&lt;/h2&gt; &lt;p&gt;If you have existing notebooks with code, you can still use the &lt;code&gt;jupyterlab-requirements&lt;/code&gt; extension to share them. The &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/invectio/"&gt;invectio&lt;/a&gt; library analyzes code in the notebook and suggests libraries that must be installed to run the notebook. Figure 8 shows an example.&lt;/p&gt; &lt;div id="attachment_882847" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05.png"&gt;&lt;img aria-describedby="caption-attachment-882847" class="wp-image-882847 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-1024x501.png" alt="There are no dependencies in the notebook metadata, but the extension identifies three packages to be installed." width="640" height="313" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-1024x501.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-300x147.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-43-05-768x376.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-882847" class="wp-caption-text"&gt;Figure 8: The &lt;code&gt;invectio&lt;/code&gt; utility identifies three packages required to run the notebook.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once again, you can just install the dependencies and start working on your project.&lt;/p&gt; &lt;h2&gt;Locking dependencies with Thoth or Pipenv&lt;/h2&gt; &lt;p&gt;The resolution engine you use to lock dependencies provides two files: a &lt;code&gt;Pipfile&lt;/code&gt; and a &lt;code&gt;Pipfile.lock&lt;/code&gt;. The &lt;code&gt;Pipfile.lock&lt;/code&gt; file states all direct and transitive project dependencies with specific versions and hashes. The notebook metadata stores these files and information about the Python version, operating system, and hardware detected. This way, anyone using the same notebook can re-create the environment that the original developer used.&lt;/p&gt; &lt;p&gt;Two resolution engines are available at the moment: &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Thoth&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/pypa/pipenv"&gt;Pipenv&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Currently, Thoth is used by default, with Pipenv as a backup. This setup guarantees that the user will receive the software stack to work on their projects. In the future, users will be able to select a specific resolution engine.&lt;/p&gt; &lt;p&gt;Using the Thoth resolution engine, you can request an optimized software stack that satisfies your requirements from the Thoth recommendation system. You can choose from the following recommendation types according to your particular needs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Latest&lt;/li&gt; &lt;li&gt;Performance&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Stable&lt;/li&gt; &lt;li&gt;Testing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information about the various &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/recommendation-types/"&gt;recommendation types&lt;/a&gt;, visit the Project Thoth website.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: The notebook metadata stores which resolution engine was used so that anyone can immediately see which one was used to resolve dependencies.&lt;/p&gt; &lt;h3&gt;Configuring the runtime environment&lt;/h3&gt; &lt;p&gt;You don’t need to worry about the runtime environment when using the Thoth resolution engine. Thoth automatically identifies the information needed to generate a recommendation and creates a &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos"&gt;Thoth configuration file&lt;/a&gt; containing the following parameters:&lt;/p&gt; &lt;pre&gt;host: {THOTH_SERVICE_HOST} tls_verify: true requirements_format: {requirements_format} runtime_environments: - name: '{os_name}:{os_version}' operating_system: name: {os_name} version: '{os_version}' hardware: cpu_family: {cpu_family} cpu_model: {cpu_model} gpu_model: {gpu_model} python_version: '{python_version}' cuda_version: {cuda_version} recommendation_type: stable platform: '{platform}'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: If you use the Thoth resolution engine, the notebook metadata will also contain information about the runtime environment used for the notebook. In this way, other data scientists using the notebook will be warned about using a different one.&lt;/p&gt; &lt;h3&gt;Installing dependencies and creating the kernel&lt;/h3&gt; &lt;p&gt;Once a lock file is created using either Thoth or Pipenv, the &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/micropipenv/"&gt;micropipenv&lt;/a&gt; tool installs the dependencies in the virtual environment. The &lt;code&gt;micropipenv&lt;/code&gt; tool supports dependency management in Python and beyond (&amp;#8220;one library to rule them all&amp;#8221;).&lt;/p&gt; &lt;p&gt;Once all of the dependencies are installed in your kernel, you are ready to work on your notebook.&lt;/p&gt; &lt;p&gt;You can choose the name of the new kernel and select the requirements from the drop-down menu. Once everything is installed, the kernel is assigned to the current notebook automatically.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements"&gt;jupyterlab-requirements&lt;/a&gt; extension is an open source project maintained by the Thoth team. We are currently exploring new features for the UI, and we welcome anyone who would like to contribute or give us feedback about the extension.&lt;/p&gt; &lt;p&gt;Have a look at the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/jupyterlab-requirements/issues"&gt;open issues&lt;/a&gt; and get in touch with the team if you like the project or if you find any issues with the extension. The Thoth team also has a &lt;a target="_blank" rel="nofollow" href="https://chat.google.com/room/AAAAVjnVXFk"&gt;public channel&lt;/a&gt; where you can ask questions about the project. We are always happy to collaborate with the community on any of &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station"&gt;our repositories&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#38;linkname=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F19%2Fmanaging-python-dependencies-with-the-thoth-jupyterlab-extension%2F&amp;#038;title=Managing%20Python%20dependencies%20with%20the%20Thoth%20JupyterLab%20extension" data-a2a-url="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/" data-a2a-title="Managing Python dependencies with the Thoth JupyterLab extension"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;Managing Python dependencies with the Thoth JupyterLab extension&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lFgYvP4zKGE" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;JupyterLab is a flexible and powerful tool for working with Jupyter notebooks. Its interactive user interface (UI) lets you use terminals, text editors, file browsers, and other components alongside your Jupyter notebook. JupyterLab 3.0 was released in January 2021. Project Thoth develops open source tools that enhance the day-to-day lives of developers and data scientists. [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/"&gt;Managing Python dependencies with the Thoth JupyterLab extension&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">855857</post-id><dc:creator>Francesco Murdaca</dc:creator><dc:date>2021-03-19T07:00:12Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/19/managing-python-dependencies-with-the-thoth-jupyterlab-extension/</feedburner:origLink></entry><entry><title type="html">Kogito Tooling 0.8.5 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/azFdIzrpPvI/kogito-tooling-0-8-5-released.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/03/kogito-tooling-0-8-5-released.html</id><updated>2021-03-19T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! On the 0.8.5 , we made many improvements and bug fixes. We are also happy to announce a new PMML Scorecard Editor and, also, that our editors are now available on Eclipse Theia Upstream (built from ). This post will give a quick overview of what is included on this release. PMML SCORECARD EDITOR (ALPHA) HITS VSCODE MARKET PLACE We are happy to announce that we have a new VS Code extension: . It allows you to create and edit PMML 4.4 (.pmml) Scorecard files. This new editor is in the alpha stage, and we are looking for feedback from the community. We hope you enjoy it! ECLIPSE THEIA AND OPEN VSIX STORE Eclipse Theia is an extensible framework based on VS Code to develop full-fledged multi-language Cloud &amp;amp; Desktop IDE-like products with state-of-the-art web technologies. Recently, Theia’s team merged a , allowing support for CustomEditor API. In practice, this means that from now on, our BPMN, DMN and editors can run on Eclipse Theia upstream (you can build it from and run), take a look on this demo: Eclipse Theia uses , and from now on, all our releases will also be available on Open VSX store. NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS We also made some new features, a lot of refactorings and improvements, with highlights to: NEW FEATURES: INFRASTRUCTURE * – Implement a integration tests using Cypress for online channel * – Migrate VS Code Extension release job to new Jenkins instance * – Converge the CSS to avoid conflicts between PF3 and PF4 EDITORS * – Score Cards: Integrate with VS Code channel * – Enable Bpmn and Dmn PR tests * – Run standalone tests with Chrome instead of Electron FIXED ISSUES IN KOGITO: INFRASTRUCTURE * – Fix running online editor integration tests in CI EDITORS * – Importing and modeling decision models is too slow for productive modeling * – [DMN Designer] Decision Services – The parameters order in the properties panel is not correct * – DMN Editor wrong edge arrow tip connection on reopen * – [DMN Designer] DMN schema/model validation errors when model has AUTO-SOURCE or AUTO-TARGET connections * – Scesim assets are broken in VS Code extension * – [DMN Designer] DMN takes too long to open models with too many nodes FURTHER READING/WATCHING We had some excellent talks recently at the KIE youtube channel: * Building successful business Java apps: How to deliver more, code less, and communicate better, by Alex; * How to embed DMN and BPMN editors in your own application, by Paulo; * Using VSCode to build and deploy services in a real-world decision scenario: COVID-19, by Adriel/ I would also like to recommend some recent articles: * , by Guilherme; * , by Matteo; * , by Eder. THANK YOU TO EVERYONE INVOLVED! I want to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look fabulous! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/azFdIzrpPvI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/kogito-tooling-0-8-5-released.html</feedburner:origLink></entry><entry><title type="html">WildFly Bootable JAR 4.0 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VEQ0cjVV9Yo/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/03/19/bootable-jar-4.0.Final-Released/</id><updated>2021-03-19T00:00:00Z</updated><content type="html">The 4.0.0.Final version of the has been released. For people who are not familiar with the WildFly Bootable JAR, I strongly recommend that you read this that covers it in detail. WILDFLY CLI SCRIPT EXECUTED AT STARTUP TIME Starting with the recently WildFly 23, the bootable JAR runtime allows you to execute a WildFly CLI script when launching the bootable JAR. Although applying changes to the server configuration at build time is the preferred way (no impact on startup time), runtime execution gives you the flexibility to adjust the server configuration to the execution context. The example has been evolved with a new 'runtime-config' Maven profile to disable CLI script execution at build time in favor of runtime execution. To execute a CLI script during boot: java -jar app-bootable.jar --cli-script=&lt;path to CLI script&gt; Note This support is Tech Preview as the mechanism may change in later releases. JBOSS MODULES MODULE ARTIFACT UPGRADES Starting with WildFly 23, you can upgrade part of the server when building a bootable JAR. This offers you the ability to use a different version of an identified component of the server (eg: an Undertow artifact, a JDBC driver provided by a third party Galleon feature-pack, … ). Obviously the updated component must be compatible with the server in which it is provisioned… This is done at your own risk ;-). WILDFLY GALLEON FEATURE-PACK SERVER ARTIFACT PACKAGING The way the JBoss Modules module artifacts that compose a WildFly server are packaged inside a WildFly Galleon feature-pack allows you to override their version when building a Bootable JAR. Instead of packaging the artifact binaries inside Galleon feature-packs, the artifacts' Maven coordinates are packaged. The actual artifact files are resolved when a WildFly server is built using Galleon (or when building a bootable JAR). This way of packaging artifacts is not new; WildFly follows this design pattern since the first Galleon releases. (This is what allows you to provision a slim WildFly server or a ). If you design custom Galleon feature-packs for WildFly, we encourage you, when applicable (the artifacts must be released in an accessible Maven repository), to design your feature-packs by following this pattern. As an example, the (that provides drivers and datasources for some major databases) pom file contains in its the driver artifacts that it can bring to the server. The JBoss Modules modules that contain the driver artifacts (e.g. ) only reference the GroupId and ArtifactId of the artifact. The artifact versions are stored inside the feature-pack but outside of the JBoss Modules module. This separation between the GroupId, ArtifactId (optionally Classifier) and the Version is what makes it possible to upgrade when building a bootable JAR. TO LEARN MORE ABOUT ARTIFACT UPGRADES The contains more information about this capability. MYFACES GALLEON FEATURE-PACK I’m happy to take the opportunity of this blog post to mention a new community that defines Galleon feature-packs that you can use with the WildFly 23 Galleon feature-pack (and "WildFly Preview" Galleon feature-pack) to build a Bootable JAR (or to provision a server using Galleon) containing a JSF implementation based on . KNOWN ISSUES INCOMPATIBILITY WITH KEYCLOAK CLIENT ADAPTER GALLEON FEATURE-PACK Due to some incompatible changes in the WildFly Galleon feature-pack (i.e. the removal of core and servlet Galleon feature-packs in the dependency chain) the Keycloak 12.0.x OIDC client adapter Galleon feature-pack can’t be used with the WildFly 23 Galleon feature-pack. As a workaround, we have setup a that highlights the workaround you need to follow to include the Keycloak 12.0.x OIDC client adapter inside a WildFly 23 bootable JAR. In summary, we are including in the bootable JAR the content of the Keycloak zipped client adapter (that you can download from ). In addition the WildFly server security is configured by the adapter-elytron-install.cli WildFly CLI script that is packaged in the zipped adapter. TO CONCLUDE Finally we would really appreciate if if you would keep us posted with your feedback and new requirements. (You can log these as new .) This will help us evolve the WildFly Bootable JAR experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VEQ0cjVV9Yo" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/19/bootable-jar-4.0.Final-Released/</feedburner:origLink></entry><entry><title type="html">RESTful Services Orchestration with Kogito and OpenAPI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aEiq4QHW-eU/restful-services-orchestration-with-kogito-and-openapi.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2021/03/restful-services-orchestration-with-kogito-and-openapi.html</id><updated>2021-03-18T19:07:06Z</updated><content type="html">The invocation of remote services plays a big role in workflow orchestration. In this blog post, we will take a look at RESTful service orchestration using and the OpenAPI specification. CNCF SERVERLESS WORKFLOW IMPLEMENTATION Kogito is a modern business automation runtime. In addition to flowchart and form-based workflow DSLs, it also supports , a declarative workflow DSL that targets the serverless technology domain. At the time of this writing, Kogito supports a subset of features of the . Since version 1.3.0, Kogito has the ability to define workflows that can via OpenAPI. This fits well with the Serverless Workflow specification, where OpenAPI is the default standard for describing RESTful services. In other words, you don’t need to worry about writing boilerplate client code to orchestrate RESTful services. All you need to do is to declare the service calls! UNDERSTANDING FUNCTION DECLARATIONS WITH OPENAPI Our business requirement is to write a simple serverless temperature converter. To do this, we want to write a workflow that can orchestrate two existing RESTful services, namely Multiplication and Subtraction services. These two services are described via OpenAPI, meaning they are described in a programming language-agnostic way. This means that you do not need to know how to write the code that invokes these services. Kogito reads this function definition during build time. It contains the needed information to generate REST client code based on these OpenAPI specification files during build time. The code generated is based on the , now embedded in our platform. In our workflow definition, we have to know the location of the services’ OpenAPI definition and the specific operation we want to invoke on the defined service. Serverless Workflow allows us to define reusable function definitions. These definitions represent an invocation of an operation on a remote service. Function definitions have a domain-specific name and can be referenced by that name throughout workflow control-flow logic when they need to actually be invoked. Below is our workflow function definition we will use throughout the blog post: "functions": [ { "name": "multiplication", "operation": "specs/multiplication.yaml#doOperation" }, { "name": "subtraction", "operation": "specs/subtraction.yaml#doOperation" } ] CALLING YOUR RESTFUL SERVICES With this in place, invoking these services in the workflow becomes trivial. All we have to do is define when in the workflow control-flow logic they need to be invoked. Workflow control-flow logic in Serverless Workflow is defined within the "states" block. This is where you define all your workflow states (steps) and the transitions between them: "states": [ { "name": "Computation", "actionMode": "sequential", "type": "operation", "actions": [ { "name": "subtract", "functionRef": { "refName": "subtraction", "parameters": { "subtractionOperation": { "leftElement": "$.fahrenheit", "rightElement": "$.subtractValue" } } } }, { "name": "multiply", "functionRef": { "refName": "multiplication", "parameters": { "multiplicationOperation": { "leftElement": "$.subtraction.difference", "rightElement": "$.multiplyValue" } } } } ] } ] Going back to our business requirements, for temperature conversion, our workflow needs to call the two services in a certain order. First, we want to execute the Multiplication service and then the Subtraction service. The is perfect for what we need. The parameters are taken from the workflow data input and processed with JSONPath expressions. And how do we know how to define these parameters? It’s just a matter of extracting from the OpenAPI Specification file: # ... operationId: doOperation requestBody: content: application/json: schema: $ref: '#/components/schemas/MultiplicationOperation' # ... components: schemas: MultiplicationOperation: type: object properties: leftElement: format: float type: number product: format: float type: number rightElement: format: float type: number The workflow declares two functions that represent the service operations that should be invoked during workflow execution. The first one, multiplication, will execute the operation doOperation from the OpenAPI specification file in our project’s classpath (Kogito also supports file and http schemas). Same thing for the subtraction function. Since this operation only needs one parameter, we can name it as we like (in this case, multiplicationOperation). For operations that require multiple parameters (like query strings), you should use the same names as defined by the OpenAPI specification. CONFIGURING THE ENDPOINTS The last piece of this puzzle is to define the URL for each of the services we want to invoke. To do so, declare the URLs in your application properties file. You should set a configuration like: org.kogito.openapi.client.&lt;spec file name&gt;.base_path=. This is a runtime property that can be defined using any method the target runtime (Quarkus or SpringBoot) supports. But if the OpenAPI Specification file declares the endpoint URL, you don’t even need to bother. Take a look at the , for example: host: petstore.swagger.io basePath: v2 schemas: - http - https Now you’re ready to call your newly generated Kogito Workflow and start orchestrating services! You can find the full Temperature Conversion workflow example . TIP: If you’re curious about the CNCF Serverless Workflow Kogito implementation, please take a look at these references: * Blog Post: ;  * Video: . If you have any questions about this new feature, please drop a question on . We would love to hear from you! * Featured photo by on The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aEiq4QHW-eU" height="1" width="1" alt=""/&gt;</content><dc:creator>Ricardo Zanini</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/restful-services-orchestration-with-kogito-and-openapi.html</feedburner:origLink></entry><entry><title type="html">How to setup the OpenShift Container Platform 4.7 on your local machine</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rdkXVj8xLsw/codeready-containers-howto-setup-openshift-47-on-local-machine.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/oyjB7WzMTt0/codeready-containers-howto-setup-openshift-47-on-local-machine.html</id><updated>2021-03-18T06:00:00Z</updated><content type="html"> Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform? Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.7? Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud native development and automated rolling deployments.  The idea was to make this as streamlined of an experience as possible by using the same  project. Let's take a look at what this looks like. Below is a walk through step by step, putting the latest OpenShift Container Platform on your local developer machine. LINUX OR MAC INSTALLATION This installation requires the following (all freely available): &gt; 1. HyperKit for OSX, Hyper-V for Windows, or Libvirt for Linux &gt; 2. Code Ready Containers (OCP 4.7) &gt; 3. OpenShift Client (oc) v4.7 First you need to ensure your virtualization tooling is installed for your platform, just search online for how to do that or your specific platform. Second you need to download the CodeReady Containers. Finally, you need the OpenShift client. Normally you'd expect to have to track these last two down but we've made this all easy by just including checks during the installation. If you have something installed, it checks the version, if good then it moves on with next steps. If anything is missing or the wrong version, the installation stops and notifies you where to find that component for your platform (including URL). Let's get started by downloading the  project and unzipping in some directory. This gives you a file called ocp-install-demo-main.zip,just unzip and run the init.sh as follows:      $ ./init.sh  Follow the instructions as each of the dependencies is checked and you're provided with pointers to getting the versions you need for your platform. Note: Each CodeReady Container download is tied to an embedded secret. This secret you need to download (link will be provided) as a file and you'll be asked to point to that secret to start your container platform. Once you've gotten all the dependencies sorted out, the install runs like this: A little ASCII art and then it's checking for my platform's virtualization (Hyperkit), then looking for the OpenShift client version (oc client), then running a setup (crc setup). The next steps are providing the pull-secret-file, you can set this in the variables at the top of the installation script. Now the moment of truth, the CodeReady Containers cluster starts, which takes some time depending on your network (crc start). With a good network it's about a five minute wait. This is the logging you'll see as the OpenShift cluster starts on your local machine. The warning is normal, just some of the features have been trimmed to speed up deployment. At the end we'll retrieve the admin password for logging in to the cluster's console, pick up the host URL, test the deployment by logging in with our client (oc login), and finally you're given all the details in a nice box. You have the option to stop, start it again, or delete the OpenShift Container Platform cluster as shown in the dialog. Next open the web console using URL and login 'kubeadmin' with the corresponding password. In our case it's the URL: https://console-openshift-console.apps-crc.testing Login with user: kubeadmin Password in our case: duduw-yPT9Z-hsUpq-f3pre That opens the main dashboard: Verify the version you are running by clicking on the top right question mark and then About option: Close the version window by clicking on the X. As we are interested in developing using the tooling and container images provided by CodeReady Containers, let's change the view from Administrator to Developer in the left top menu selecting Topology and then via Project drop down menu at the top choose Default: You can browse the offerings in the provided container catalog by selecting From Catalog and then for example, Middleware to view the offerings available: Looking to get started with an example usage, try the  or examples that leverage the provided developer catalog container images. You can also explore how an existing project is set up using one of the developer catalog container images with a . This concludes the installation and tour of an OpenShift Container Platform on our local machine using CodeReady Containers. WHAT ABOUT WINDOWS? If you are a sharp observer, you'll notice there is a file called init.bat for windows platforms to install with. The problem is I've not been able to test this yet on a windows machine, so I'd love to call out to the readers out there that might have some time to contribute to test this script and help us complete the installation. You'll notice a few TODO's marked in the scripts code, as they are untested areas in the installation. You can  and help us complete the windows based installation and get your name added to the contributors list. We'd be really thankful! If you are interested in the , see this post for details on how to install. Stay tuned for more on cloud-native development using other Red Hat technologies on your new OpenShift Container Platform installed locally on your own machine!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rdkXVj8xLsw" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/oyjB7WzMTt0/codeready-containers-howto-setup-openshift-47-on-local-machine.html</feedburner:origLink></entry><entry><title>Using Dekorate to generate Kubernetes manifests for Java applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/28qOSk7b5HQ/" /><category term="DevOps" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Dekorate" /><category term="deployment" /><category term="kubernetes manifest" /><category term="openshift" /><author><name>Aurea Munoz Hernandez</name></author><id>https://developers.redhat.com/blog/?p=861907</id><updated>2021-03-17T07:00:09Z</updated><published>2021-03-17T07:00:09Z</published><content type="html">&lt;p&gt;To deploy an application on &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; or &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, you first need to create &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/"&gt;objects&lt;/a&gt; to allow the platform to install an application from a container image. Then, you need to launch the application using a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;pod&lt;/a&gt; and expose it as a service with a static IP address. Doing all of that can be tedious, but there are ways to simplify the process.&lt;/p&gt; &lt;p&gt;Kubernetes follows a &lt;em&gt;declarative model&lt;/em&gt;, meaning that the user declares the desired application state and the cluster adjusts to match. Developers use files called &lt;em&gt;manifests&lt;/em&gt; to describe the desired state. Manifests are typically defined in YAML or JSON files, which are communicated to the server through its &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/"&gt;REST API endpoint&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Object formats are complex, with many fields to manipulate. It&amp;#8217;s a good idea to use a tool to help with creating the manifests. If you’re deploying &lt;a target="_blank" rel="nofollow" href="/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications, consider using &lt;a target="_blank" rel="nofollow" href="http://dekorate.io"&gt;Dekorate&lt;/a&gt;. Not only will it simplify your work as a developer, but it will also flatten your learning curve as you adopt Kubernetes.&lt;/p&gt; &lt;p&gt;In this article, we&amp;#8217;ll use Dekorate to generate Kubernetes and OpenShift manifests for a generic Java application. Our example is a simple REST API application.&lt;/p&gt; &lt;h2&gt;Making Java projects easier with Dekorate&lt;/h2&gt; &lt;p&gt;The Dekorate project offers a collection of Java annotations and processors that automatically update Kubernetes manifests during your application’s compilation. Developers can customize the manifests using either annotations or properties in configuration files, without the hassle of editing individual XML, YAML, or JSON templates.&lt;/p&gt; &lt;p&gt;Each annotation field maps to a property, so anything you specify with Java annotations can also be specified with properties, like so: &lt;code&gt;dekorate.[simplified-annotation-name].[&lt;a target="_blank" rel="nofollow" href="https://medium.com/better-programming/string-case-styles-camel-pascal-snake-and-kebab-case-981407998841"&gt;kebab-cased-property-name&lt;/a&gt;]&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The simplified name corresponds to the annotation’s class name, set as lowercase and stripped of suffixes (for example, &lt;code&gt;application&lt;/code&gt;). You can find a complete reference of the supported properties &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate/blob/master/assets/config.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Dekorate works with practically any Java build tool, including Maven, Gradle, Bazel, and sbt. To make life even easier, Dekorate also detects Java frameworks such as &lt;a target="_blank" rel="nofollow" href="/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="/topics/quarkus/"&gt;Quarkus&lt;/a&gt;, and Thorntail, aligning the generated manifests accordingly. See the article &lt;a target="_blank" rel="nofollow" href="/blog/2019/08/15/how-to-use-dekorate-to-create-kubernetes-manifests/"&gt;&lt;em&gt;How to use Dekorate to create Kubernetes manifests&lt;/em&gt; for more details&lt;/a&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate/blob/master/examples"&gt;Try out these examples&lt;/a&gt; if you want to play with Dekorate and discover the power of this fantastic tool for yourself.&lt;/p&gt; &lt;h2&gt;A simple REST application in Java&lt;/h2&gt; &lt;p&gt;Dekorate was designed to work with several Java frameworks, but it works just as well with plain Java. In the next sections, we&amp;#8217;ll use Dekorate to generate Kubernetes and OpenShift manifests for a generic Java application. Our example is a simple REST API. You can &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless"&gt;find the complete source code on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Deployment objects&lt;/h3&gt; &lt;p&gt;There are three Kubernetes objects Dekorate must generate to deploy an application. Each of these objects—and their OpenShift platform equivalents—plays a different role, as described in Table 1.&lt;/p&gt; &lt;table align="”center”"&gt; &lt;caption&gt;&lt;strong&gt;Table 1: Objects used to deploy applications on Kubernetes and OpenShift.&lt;/strong&gt;&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Kubernetes resource&lt;/th&gt; &lt;th&gt;OpenShift resource&lt;/th&gt; &lt;th&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-and-deploymentconfigs_what-deployments-are"&gt;DeploymentConfig&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Deploy and update applications as a pod.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/core_concepts/pods_and_services.html#services"&gt;Service&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Provide an endpoint to access the pod as a service.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/networking/routes.html"&gt;Route&lt;/a&gt;&lt;/td&gt; &lt;td&gt;Expose the service to clients outside of the cluster.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Next, we&amp;#8217;ll review the steps for creating manifests with Dekorate so that they include the resources listed in Table 1.&lt;/p&gt; &lt;h3&gt;Configuring the manifests&lt;/h3&gt; &lt;p&gt;The easiest way to enable Dekorate is to add the corresponding JAR file to the classpath using a Maven dependency:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62;     &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62;     &amp;#60;artifactId&amp;#62;kubernetes-annotations&amp;#60;/artifactId&amp;#62;      &amp;#60;version&amp;#62;0.13.6&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To keep your projects up to date, make sure you use &lt;a href="https://github.com/dekorateio/dekorate/releases"&gt;the latest version of Dekorate&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We need to specify that we would like to use Dekorate during the compilation phase of the Maven build life cycle. We can do this by adding the &lt;code&gt;@Dekorate&lt;/code&gt; annotation to our main Java class. Here, we&amp;#8217;ll use the &lt;code&gt;@KubernetesApplication&lt;/code&gt; annotation, which provides Kubernetes-specific configuration options. Edit the &lt;code&gt;App.java&lt;/code&gt; class and add the annotation:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication( . . .       name = "hello-world-fwless-k8s",        ... )&lt;/pre&gt; &lt;p&gt;This configuration creates a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt; during the Maven compile goal. Dekorate uses the &lt;code&gt;name&lt;/code&gt; parameter value to specify the resource name and populate the corresponding Kubernetes label (&lt;code&gt;app.kubernetes.io/name&lt;/code&gt;). The &lt;code&gt;.yml&lt;/code&gt; and &lt;code&gt;.json&lt;/code&gt; manifest files are created in the &lt;code&gt;target/classes/META-INF/dekorate/*.{json,yml}&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;As I mentioned in the introduction, Dekorate has a lightweight integration with build automation systems like Maven and Gradle. As a result, it can read the information from the tool configuration without bringing the build tool itself into the classpath. In our example, Dekorate would have used the name &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dc58a3358f1e6618ff9b743803c521d8411bfacc/pom.xml#L5"&gt;maven artifactId&lt;/a&gt; by default, but we overrode it with the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dekorate-4-k8s/src/main/java/org/acme/App.java#L21"&gt;annotation name parameter&lt;/a&gt;. Dekorate will add this name as &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/"&gt;Kubernetes labels&lt;/a&gt; to all resources, including images, containers, deployments, and services.&lt;/p&gt; &lt;h3&gt;The annotation port parameter&lt;/h3&gt; &lt;p&gt;The next step is to add the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/cea00f8199f8c2bf04fb431b0766a701a5f6ce62/src/main/java/org/acme/App.java#L22"&gt;annotation port parameter&lt;/a&gt;. This lets us configure the &lt;code&gt;Service&lt;/code&gt; endpoint and specify how it should be mapped with the Java HTTP port:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...     ports = @Port(name = "web", containerPort = 8080),     ... )&lt;/pre&gt; &lt;p&gt;Configuring the annotation port parameter adds the container port within the &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt; resource. That also generates a &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt; resource that allows access to the endpoint inside the Kubernetes cluster. Once again, if you use a framework like Spring Boot or Quarkus, you can bypass the port definition. Dekorate will use the existing &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate#framework-integration"&gt;framework metadata&lt;/a&gt; to generate the resources.&lt;/p&gt; &lt;h3&gt;The Ingress resource&lt;/h3&gt; &lt;p&gt;Having a service or endpoint available internally within the cluster is great. But to simplify your life, we will now configure an additional &lt;code&gt;Ingress&lt;/code&gt; resource to make the service visible from your laptop. Both resources configure the proxy application (the Ingress controller) on the cluster to redirect external traffic and internal applications.&lt;/p&gt; &lt;p&gt;Add the following &lt;code&gt;expose&lt;/code&gt; parameter to the Dekorate annotation:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...     expose = true,     ... )&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Remember that if you modify the annotation parameters, you can verify that the change is reflected in the manifest by triggering the compilation and checking the value gathered in the manifest file.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll add a &lt;code&gt;host&lt;/code&gt; parameter to specify the local IP address or DNS-resolvable hostname to access the service:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...      host = "fw-app.127.0.0.1.nip.io",    ... )&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;host&lt;/code&gt; determines the host under which the application will be exposed. The &lt;code&gt;Ingress&lt;/code&gt; &lt;a target="_blank" rel="nofollow" href="https://v1-16.docs.kubernetes.io/docs/concepts/architecture/controller/"&gt;controller&lt;/a&gt; running on the Kubernetes cluster uses the &lt;code&gt;host&lt;/code&gt; to add a new rule to forward the requests addressed to that host to the corresponding service (for example, &lt;code&gt;fw-app&lt;/code&gt;).&lt;/p&gt; &lt;h3&gt;The Deployment controller&lt;/h3&gt; &lt;p&gt;Finally, we need to tell the &lt;code&gt;Deployment&lt;/code&gt; controller how to behave when a new image is available on the container registry. Setting the &lt;code&gt;ImagePullPolicy&lt;/code&gt; parameter to &lt;code&gt;Always&lt;/code&gt; ensures the controller will redeploy a new application as soon as a new container image is available:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     ...      imagePullPolicy = ImagePullPolicy.Always,     ... )&lt;/pre&gt; &lt;h2&gt;Deploying the application&lt;/h2&gt; &lt;p&gt;To sum up, here&amp;#8217;s the Dekorate configuration using the Java annotation parameters we&amp;#8217;ve discussed:&lt;/p&gt; &lt;pre&gt;@KubernetesApplication(     name = "hello-world-fwless-k8s",  ports = @Port(name = "web", containerPort = 8080),      expose = true, host = "fw-app.127.0.0.1.nip.io", imagePullPolicy = ImagePullPolicy.Always ) &lt;/pre&gt; &lt;p&gt;The generated manifests will appear in the &lt;code&gt;target/classes/META-INF/dekorate&lt;/code&gt; folder as &lt;code&gt;kubernetes.yml&lt;/code&gt; and &lt;code&gt;kubernetes.json&lt;/code&gt; files.&lt;/p&gt; &lt;p&gt;Alternatively, you can deploy the application on OpenShift using the OpenShift-specific dependency. Edit the &lt;code&gt;pom.xml&lt;/code&gt; and add the following:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;openshift-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;0.13.6&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;Now, edit the &lt;code&gt;App.java&lt;/code&gt; class and add the &lt;code&gt;@OpenShiftApplication&lt;/code&gt; annotation:&lt;/p&gt; &lt;pre&gt;@OpenshiftApplication(     name = "hello-world-fwless-openshift",  ports = @Port(name = "web", containerPort = 8080),      expose = true, imagePullPolicy = ImagePullPolicy.Always )&lt;/pre&gt; &lt;p&gt;This time, the manifests are named &lt;code&gt;openshift.yml&lt;/code&gt; and &lt;code&gt;openshift.json&lt;/code&gt;, and they will contain the OpenShift resources listed in Table 1: &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html#deployments-and-deploymentconfigs_what-deployments-are"&gt;DeploymentConfig&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/core_concepts/pods_and_services.html#services"&gt;Service&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/online/pro/architecture/networking/routes.html"&gt;Route&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Configuring the image build strategy&lt;/h2&gt; &lt;p&gt;Running applications on Kubernetes and OpenShift requires that we package them into a container image that the pod will bootstrap at launch time. The process of packaging the source code within a container image, container registry, or image name is what we call the &lt;em&gt;image build strategy&lt;/em&gt;. Dekorate currently supports several modes, depending on whether you use Kubernetes or OpenShift.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s see how to configure the image build strategy with Dekorate.&lt;/p&gt; &lt;h3&gt;Docker (Kubernetes and OpenShift)&lt;/h3&gt; &lt;p&gt;To configure the Docker image build strategy with Dekorate to use the Docker client as a build tool locally, add the following dependency to the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62;  &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;docker-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;If no registry is specified, Docker.io will be used by default. Recompile the project using Maven or Gradle. The generated manifest file will include the container image as part of the &lt;code&gt;Deployment&lt;/code&gt; or &lt;code&gt;DeploymentConfig&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt;image: amunozhe/hello-world-fwless:1.0-SNAPSHOT&lt;/pre&gt; &lt;p&gt;These parameters will be used when you launch the tool responsible for building the container image. We&amp;#8217;ll examine this in more detail &lt;a href="#creating_and_sharing"&gt;later&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to use an alternative image registry, edit the &lt;code&gt;App.java&lt;/code&gt; class to add the &lt;code&gt;@DockerBuild&lt;/code&gt; annotation to specify the &lt;code&gt;registry&lt;/code&gt; where the client will push the image:&lt;/p&gt; &lt;pre&gt;@DockerBuild(registry = "quay.io")&lt;/pre&gt; &lt;h3&gt;Source-to-Image (S2I) (OpenShift only)&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/builds/understanding-image-builds.html#build-strategy-s2i_understanding-image-builds"&gt;Source-to-Image (S2I)&lt;/a&gt; is a tool for building Docker-formatted images on OpenShift. With S2I, an OpenShift controller performs the image build, so you don’t have to build the container image locally.&lt;/p&gt; &lt;p&gt;Until recently, Dekorate only supported S2I for image builds; the S2I resource &lt;code&gt;BuildConfig&lt;/code&gt; is generated by Dekorate out of the box when the &lt;code&gt;@OpenShiftApplication&lt;/code&gt; annotation is used. To bypass generating the &lt;code&gt;BuildConfig&lt;/code&gt; resource, you can use the &lt;code&gt;@S2iBuild(enabled=false)&lt;/code&gt; parameter.&lt;/p&gt; &lt;h2&gt;Generating the manifests&lt;/h2&gt; &lt;p&gt;Once we&amp;#8217;ve configured the Kubernetes or OpenShift manifests, we can execute a &lt;code&gt;mvn package&lt;/code&gt; command to generate the actual manifests. As already mentioned, two files will be created in the &lt;code&gt;target/classes/META-INF/dekorate&lt;/code&gt; directory: &lt;code&gt;kubernetes.json&lt;/code&gt; and &lt;code&gt;kubernetes.yml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, we are ready to build the container image according to the specified image build strategy.&lt;/p&gt; &lt;h2 id="creating_and_sharing"&gt;Creating and sharing the container image&lt;/h2&gt; &lt;p&gt;Nowadays, many technologies exist to build container images locally. This example introduces two of them: &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/engine/reference/commandline/image_build/"&gt;Docker&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/GoogleContainerTools/jib"&gt;Jib&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Docker&lt;/h3&gt; &lt;p&gt;A Dockerfile is a text file used to build an image with Docker or &lt;a target="_blank" rel="nofollow" href="https://podman.io/"&gt;Podman&lt;/a&gt;. It contains instructions for containerizing your Java application. A &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/dekorate-4-k8s/Dockerfile"&gt;Dockerfile&lt;/a&gt; is provided with the example project&amp;#8217;s source code and is available in the project&amp;#8217;s root directory.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Dekorate does not generate Dockerfiles. It also doesn’t provide internal support for performing image builds and pushes.&lt;/p&gt; &lt;p&gt;If you need to install the Docker client locally, you can &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/get-docker/"&gt;download it here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Run the following command to build the image:&lt;/p&gt; &lt;pre&gt;docker build -f Dockerfile -t amunozhe/hello-world-fwless:1.0-SNAPSHOT .&lt;/pre&gt; &lt;p&gt;This image is only available within the Docker registry on your laptop. We need to push the image to an external image registry to allow Kubernetes or OpenShift to run it. For this example, we&amp;#8217;ll push the image to Docker Hub using my Docker Hub ID (amunozhe). You can register your own ID on the &lt;a target="_blank" rel="nofollow" href="http://hub.docker.com"&gt;Docker Hub website&lt;/a&gt; or use another publicly-available registry like &lt;a target="_blank" rel="nofollow" href="https://quay.io/"&gt;Red Hat Quay&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Once you’re logged in to the registry, you can finally push the image to make it available for the cluster:&lt;/p&gt; &lt;pre&gt;docker push amunozhe/hello-world-fwless:1.0-SNAPSHOT&lt;/pre&gt; &lt;h4&gt;Build and push the image via Dekorate&lt;/h4&gt; &lt;p&gt;Instead of manually executing the commands to build and push the Java container image, you can delegate the task to Dekorate, which uses &lt;a target="_blank" rel="nofollow" href="https://github.com/dekorateio/dekorate#docker-build-hook"&gt;hooks&lt;/a&gt; to support such features. This means you can perform all of the necessary steps with a single command:&lt;/p&gt; &lt;pre&gt;mvn clean package -Ddekorate.build=true  -Ddekorate.push=true&lt;/pre&gt; &lt;h3&gt;Jib&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin"&gt;Jib&lt;/a&gt; simplifies the process of creating container images by letting you avoid writing a Dockerfile. You don’t even need to have a Docker client installed to create and publish container images with Jib. Using Jib (via a Maven plugin) is also nice because it catches any changes made to the application every time you build.&lt;/p&gt; &lt;p&gt;To enable Jib, replace the &lt;code&gt;docker-annotations&lt;/code&gt; dependency with &lt;code&gt;jib-annotations&lt;/code&gt; in the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.dekorate&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;jib-annotations&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;We&amp;#8217;ll use the &lt;code&gt;@JibBuild&lt;/code&gt; annotation instead of &lt;code&gt;@DockerBuild&lt;/code&gt; to configure the registry. Edit the &lt;code&gt;application.java&lt;/code&gt; class to add the annotation:&lt;/p&gt; &lt;pre&gt;@JibBuild(registry = "docker.io")&lt;/pre&gt; &lt;p&gt;Run the following command to build and push the container image, taking advantage of the Dekorate hook:&lt;/p&gt; &lt;pre&gt;mvn clean package -Ddekorate.push=true&lt;/pre&gt; &lt;h2&gt;Deploying the application&lt;/h2&gt; &lt;p&gt;Once the Kubernetes and OpenShift manifests are generated, and the image is pushed to an image registry, we can deploy the application on the cluster under the demo &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;namespace&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To create your microservice on a cloud platform, you must have access to an active Kubernetes or OpenShift cluster. Here is the command to execute for Kubernetes:&lt;/p&gt; &lt;pre&gt;kubectl create ns demo kubectl apply -f target/classes/META-INF/dekorate/kubernetes.yml -n demo&lt;/pre&gt; &lt;p&gt;And here&amp;#8217;s the one for OpenShift:&lt;/p&gt; &lt;pre&gt;oc new-project demo oc apply -f target/classes/META-INF/dekorate/openshift.yml &lt;/pre&gt; &lt;p&gt;Wait a few moments until the pod is created and accessible through the external URL registered as an &lt;code&gt;Ingress&lt;/code&gt; or &lt;code&gt;Route&lt;/code&gt; resource. You can verify that the application is ready to accept the request by checking to see if the pod status is &lt;code&gt;RUNNING&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Kubernetes users should use this command:&lt;/p&gt; &lt;pre&gt;kubectl get pods -n demo&lt;/pre&gt; &lt;p&gt;For OpenShift users, it is:&lt;/p&gt; &lt;pre&gt;oc project demo oc get pods&lt;/pre&gt; &lt;p&gt;Next, you need to get the URL to access the application in the browser. Use the following command for Kubernetes:&lt;/p&gt; &lt;pre&gt;kubectl get ingress -n demo NAME                     CLASS HOSTS                     ADDRESS     PORTS AGE hello-world-fwless-k8s   &amp;#60;none&amp;#62;  fw-app.127.0.0.1.nip.io    localhost   80      147m&lt;/pre&gt; &lt;p&gt;As you can see in the &lt;a target="_blank" rel="nofollow" href="https://github.com/aureamunoz/hello-world-fwless/blob/master/src/main/java/org/acme/App.java#L14"&gt;main Application class&lt;/a&gt;, the &lt;code&gt;/api/hello&lt;/code&gt; path provides the endpoint. To check if the application is accessible, open your browser and go to &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="http://fw-app.127.0.0.1.nip.io/api/hello"&gt;http://fw-app.127.0.0.1.nip.io/api/hello&lt;/a&gt;&lt;/code&gt;. You should see the following message:&lt;/p&gt; &lt;pre&gt;Hello From k8s FrameworkLess world!&lt;/pre&gt; &lt;p&gt;Here is how to check the application with OpenShift:&lt;/p&gt; &lt;pre&gt;oc get route -n demo NAME               HOST/PORT                             PATH  PORT hello-world-fwless-openshift  hello-world-fwless-openshift-demo.88.99.12.170.nip.io   /        8080&lt;/pre&gt; &lt;p&gt;In the browser, go to &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="http://hello-world-fwless-openshift-demo.88.99.12.170.nip.io/api/hello"&gt;http://hello-world-fwless-openshift-demo.88.99.12.170.nip.io/api/hello&lt;/a&gt;&lt;/code&gt;. You should see the following message:&lt;/p&gt; &lt;pre&gt;Hello from OpenShift FrameworkLess world!&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;As a developer, I love creating new features, improving user experiences, and discovering ways to deploy applications quickly and painlessly. As you&amp;#8217;ve seen in this article, Dekorate makes writing Kubernetes or OpenShift manifests practically effortless.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#38;linkname=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F17%2Fusing-dekorate-to-generate-kubernetes-manifests-for-java-applications%2F&amp;#038;title=Using%20Dekorate%20to%20generate%20Kubernetes%20manifests%20for%20Java%20applications" data-a2a-url="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/" data-a2a-title="Using Dekorate to generate Kubernetes manifests for Java applications"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/"&gt;Using Dekorate to generate Kubernetes manifests for Java applications&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/28qOSk7b5HQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;To deploy an application on Kubernetes or Red Hat OpenShift, you first need to create objects to allow the platform to install an application from a container image. Then, you need to launch the application using a pod and expose it as a service with a static IP address. Doing all of that can be tedious, [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/"&gt;Using Dekorate to generate Kubernetes manifests for Java applications&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">861907</post-id><dc:creator>Aurea Munoz Hernandez</dc:creator><dc:date>2021-03-17T07:00:09Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/</feedburner:origLink></entry><entry><title type="html">Time series component for Dashbuilder</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2_WfA1oWiwQ/time-series-component-for-dashbuilder.html" /><author><name>Manaswini Das</name></author><id>https://blog.kie.org/2021/03/time-series-component-for-dashbuilder.html</id><updated>2021-03-16T18:37:38Z</updated><content type="html">As you probably already know, you can use, a part of Business Central to create pages and intuitive dashboards. In Dashbuilder, Pages are composed of small components that can show any type of data. Dashbuilder provides by default multiple components that users can drag to pages. Recently, we have added a bunch of new components like treemaps, charts, maps, etc, to extend the usability and aid in representing user data in a precise way. Recently, we added the Prometheus dataset provider to extend the usability of Dashbuilder to represent time series metrics (see more details on this ). In this blog post, I’m going to walk you through the new external component added to Dashbuilder to better represent time-series data and use it to create your own dashboards connected to your time series datasets. TIME SERIES COMPONENT This is one of the new components that we have added using React and library. Now, you can provide a custom dataset or Prometheus metrics and create visualizations of your time series data on a line or area chart using Dashbuilder. ApexCharts is an MIT licensed open-source library to create interactive JavaScript charts built on SVG. You can find it on. ApexCharts provides some in-built features like downloading the dataset in CSV or downloading the chart in PNG or SVG format. You just have to click the sandwich menu icon on the top right corner to discover it. The component that I have used is zoomable time series, which means you can choose to zoom in and out a particular area of the chart. To use the time series component on Dashbuilder, get the component from and paste it in the Components directory. For more information about Developing custom components, head over to . After adding the component to the aforementioned directory and enabling external components, just click on the External Components dropdown, and select time-series-chart and drag it to the page, select the dataset in the Data tab(make sure that the columns are selected properly), set the component properties in the Component Editor tab and you are done. In order to add datasets, click on the menu dropdown in the navigation bar, select Datasets. You will be asked the type of dataset you want to add. You can add a CSV dataset or Prometheus metrics according to your choice. Based on what you name the datasets while adding them, the names of datasets in the Data tab will be populated after dragging the component to the page. You can use any library of your choice to create a component, just add the library, for instance, react-apexcharts and apexcharts to package.json and import them in respective TypeScript or JavaScript files. Configure the data to the required format to feed it to the component, for example, the major props that the Zoomable series chart uses includes options and series. The Options interface, which takes care of the x-axis categories and the chart name and the Series interface, which takes the arrays of names and the series values, looks like this: View the code on . We have the following component properties to make it customizable: * Show Area: A checkbox to set the type of chart, area, or line; * Chart Name: To set the chart name; * Date Categories: A checkbox to handle categories as dates or pure text; * Labels: To enable or disable data labels on data points; * Transposed: Whether the dataset provided uses series as separate columns or as rows. Time series component in action CONCLUSION There is no limit to what library/framework you want to use. We are continuing to include more custom components to allow users to create interactive dashboards. Huge shoutout to for coming up with integrating the Prometheus dataset provider with Dashbuilder and for the chart GIF. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2_WfA1oWiwQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Manaswini Das</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/time-series-component-for-dashbuilder.html</feedburner:origLink></entry><entry><title type="html">Apache Camel 3.9 - No more saw tooth JVM garbage collection</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/S2XnFEGUmug/apache-camel-39-no-more-saw-tooth-jvm.html" /><author><name>Claus Ibsen</name></author><id>http://feedproxy.google.com/~r/ApacheCamel/~3/D69y7h3uZWA/apache-camel-39-no-more-saw-tooth-jvm.html</id><updated>2021-03-16T13:07:00Z</updated><content type="html">We continue our effort to optimize Apache Camel. This is blog post part 7 which covers are latest effort on dramatically reducing the object allocations caused by Camel while routing messages. The good news is that we have overachieved and was able to reduce object allocations to ZERO!!! - so no more JVM memory usage graphs with saw tooth (note: in real world use-cases there will always be user data causing object allocations - but I wanted to have a click-bait blog title). To help identify potential areas of improvement in the core Camel, we put together a , which has only a single route triggered by a timer every producing 1000 msg/sec. These messages are routed to 10 different log endpoints (logging turned off). This allows us to focus on the internals of Camel only and what code paths is executed and what objects are being allocated and in-use by the internal routing engine. There are no message data (body or headers), or network communication etc.  Running the example (JVM heap size set to max 32mb) for 10 minutes profiled by JFR and browsed in JDK mission control we can see the dramatic difference.  In Camel 3.8 597mb of objects is allocated by Camel in total. And in Camel 3.9 that is ZERO. How did we get to zero? That is a long journey that started about a year ago, and we have gradually optimised Camel which I have blogged about in the 6 parts preceding this post. All this work is like pealing an onion, layer after layer. As one layer has been optimised, then the profiler reveals another layer, and so on. This time we could identify 5 areas for improvements: * consumers * core EIP patterns * internal routing processor * error handler * exchange and message The consumers are the source of incoming messages into Apache Camel. And so that is a great place to start. It's the consumers that allocate a new exchange, populate the exchange with message data such as body and headers.  After that it's the internal routing engine that routes the exchange via EIP patterns. And here we identified several spots where we could eliminate object allocations, or reduce allocations when some features are not in use etc. Error handling is one of the most complex part in the core Camel, and it uses objects to keep state in case of exceptions to handle redeliveries and whatnot. We were able to split the error handling into two tasks that operate either as a simplified or complex task. In the core EIP patterns we were able to optimize code that reduces object allocations. The 5th area we optimized is the exchange object. EIPs and the Camel routing engine store state per exchange on the exchange instance itself as exchange properties. That data is stored in a Map which means for each entry both a key is allocated in the java.util.Map. We optimized this to use an internal object array where each key is hardcoded as an index entry in the array. That means read/write is very fast and simple as its just an array index.  And then we ..... cheated ... instead of allocating new objects (via new constructor) we recycle existing objects from the previous exchange to the next. In other words we are using a sort of object pooling - this feature is called in Camel. Exchange Pooling The diagram above with ZERO object allocation is in fact with exchange pooling enabled. If exchange pooling is turned off (default), then the diagram should have been as below: As you can see there is saw-tooth graph. However the total object allocation is gone down from 597mb to 492mb (18% reduction). Awesome this is fantastic. And yes indeed it is. However when using anything there are both pros and cons, and so with object pooling. There is tiny tiny overhead of Camel to manage the object pools, and to "scrub" objects before they can be recused. That is a possibly a very very tiny CPU overhead compared to the JVM allocate and initialise new objects; instead of pool reuse. The biggest con is object leaks .. if objects are no returned back in the pool. Therefore you can turn on statistics which will report a WARN if a leak is detected when you stop Camel. The objects must be manually returned back into the pool, which we have coded in all the Camel components, and of course in core Camel. Now object leaks in this situation is not severe as you just have a situation as if there are no pooling, the JVM will create a new object - so the object allocations goes up, but its not severe like a database pool leaking TCP network connections. Upcoming work There are a few very complex EIP patterns and Camel component which does not yet support object pooling. We have this on the roadmap for Camel 3.10. Camel 3.9 is planned for release in March 2021.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/S2XnFEGUmug" height="1" width="1" alt=""/&gt;</content><dc:creator>Claus Ibsen</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/D69y7h3uZWA/apache-camel-39-no-more-saw-tooth-jvm.html</feedburner:origLink></entry><entry><title>Three ways to containerize .NET applications on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hKLJqjbCuFA/" /><category term=".NET Core" /><category term="Containers" /><category term="Kubernetes" /><category term="Linux" /><category term="Windows" /><category term=".NET OpenShift" /><category term="Linux containers" /><category term="OpenShift CNV" /><category term="Windows containers" /><author><name>Don Schenck</name></author><id>https://developers.redhat.com/blog/?p=855247</id><updated>2021-03-16T07:00:05Z</updated><published>2021-03-16T07:00:05Z</published><content type="html">&lt;p&gt;When Microsoft &lt;a target="_blank" rel="nofollow" href="https://devblogs.microsoft.com/dotnet/net-core-is-open-source/"&gt;announced in November 2014&lt;/a&gt; that the &lt;a target="_blank" rel="nofollow" href="/topics/dotnet"&gt;.NET Framework&lt;/a&gt; would be &lt;a target="_blank" rel="nofollow" href="/topics/open-source/"&gt;open source&lt;/a&gt;, the .NET developer&amp;#8217;s world shifted. This was not a slight drift in a new direction; it was a tectonic movement with huge implications.&lt;/p&gt; &lt;p&gt;For one thing, it positioned .NET developers to take part in the rapidly-growing world of Linux &lt;a target="_blank" rel="nofollow" href="/topics/containers/"&gt;containers&lt;/a&gt;. As container development matured to include technologies such as &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;Knative&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, .NET developers faced the opportunity—and challenge—of moving existing applications to use &lt;a target="_blank" rel="nofollow" href="/topics/microservices/"&gt;microservices&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containers&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Fortunately, we have options. This article describes three paths for .NET development for OpenShift: Linux containers, Windows containers, and OpenShift container-native virtualization.&lt;/p&gt; &lt;h2&gt;Linux is containers, containers are Linux&lt;/h2&gt; &lt;p&gt;.NET Core is the most obvious and direct route to .NET in containers. Containerizing a .NET Core application is as simple as writing a Dockerfile and building the image using Podman or Docker. While it does require either a Linux-based computer or the proper Windows-based environment (Hyper-V or Windows Subsystem for Linux 2), no special steps or OpenShift technologies are needed. Build the image and import it to OpenShift.&lt;/p&gt; &lt;p&gt;Using &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; containers would seem to be the preferred route for new (&amp;#8220;greenfield&amp;#8221;) applications because no porting or rewriting is needed. If you want to port an existing application, two tools can help: Microsoft&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/Microsoft/dotnet-apiport"&gt;ApiPort&lt;/a&gt; and Amazon&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://aws.amazon.com/porting-assistant-dotnet/"&gt;Porting Assistant for .NET&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to start writing .NET applications to run in containers while keeping your existing monolith running in Windows, I suggest learning about the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer-demos/cloud-native-compass/blob/master/strangler-pattern.md"&gt;strangler pattern&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to start a new application, or if you can easily port your current application, or if you want to employ the strangler pattern, .NET Core running Linux containers is the way to go.&lt;/p&gt; &lt;p&gt;But what if you want to move an existing .NET Framework application immediately?&lt;/p&gt; &lt;h2&gt;Windows containers&lt;/h2&gt; &lt;p&gt;The long-awaited &lt;a href="https://developers.redhat.com/blog/category/windows/"&gt;Windows&lt;/a&gt; containers technology is here and ready for you to run with it on OpenShift. Using the same management model—and the same management plane—as Linux containers, Windows containers allow you to build and run images on the Windows operating system.&lt;/p&gt; &lt;p&gt;What is the practical takeaway of this? How about the ability to run .NET Framework applications in containers? Yes, &lt;em&gt;Framework&lt;/em&gt;. Not Core; Framework. Let your imagination run wild with that for a moment. Build a Windows container image, and running your IIS website can be as easy as &lt;code&gt;docker run -d -p 80:80 my-iis-website&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;There are, of course, a few things to consider when using this approach. I will explore these points in an upcoming article about Windows containers on OpenShift. Stay tuned.&lt;/p&gt; &lt;h2&gt;OpenShift CNV&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization"&gt;OpenShift container-native virtualization&lt;/a&gt;, or CNV, is built on the &lt;a target="_blank" rel="nofollow" href="https://kubevirt.io/"&gt;KubeVirt&lt;/a&gt; technology. The KubeVirt project lets you control a virtual machine (VM) with Kubernetes, treating it like a container. The many advantages of this technology, coupled with OpenShift, include immediate lift-and-shift without any code changes, role-based access control (RBAC) and networking policies as afforded by OpenShift, and more. In short, you get all the advantages of Kubernetes and OpenShift while keeping your Windows VM running.&lt;/p&gt; &lt;p&gt;If you want to move your legacy .NET Framework applications to OpenShift and benefit from its features &lt;em&gt;immediately&lt;/em&gt;, this route could be the best. It buys you time as you evaluate your path forward, which might include containers—Windows, Linux, or both.&lt;/p&gt; &lt;h2&gt;More to come&lt;/h2&gt; &lt;p&gt;This article was only a brief introduction to the latest technologies for containerizing .NET applications. Your individual path depends on your situation, budget, time, and so on. To help make the decision easier, look for three more articles on this topic—one for each path: Linux containers, Windows containers, and OpenShift CNV with a Windows VM.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#38;linkname=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F16%2Fthree-ways-to-containerize-net-applications-on-red-hat-openshift%2F&amp;#038;title=Three%20ways%20to%20containerize%20.NET%20applications%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/" data-a2a-title="Three ways to containerize .NET applications on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;Three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hKLJqjbCuFA" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;When Microsoft announced in November 2014 that the .NET Framework would be open source, the .NET developer&amp;#8217;s world shifted. This was not a slight drift in a new direction; it was a tectonic movement with huge implications. For one thing, it positioned .NET developers to take part in the rapidly-growing world of Linux containers. As [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;Three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">855247</post-id><dc:creator>Don Schenck</dc:creator><dc:date>2021-03-16T07:00:05Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/</feedburner:origLink></entry><entry><title type="html">Supply chain integration - Example store integration architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VbDHkj9ufWE/supply-chain-integration-example-store-integration-architecture.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/57bT5V1qM3Y/supply-chain-integration-example-store-integration-architecture.html</id><updated>2021-03-16T06:00:00Z</updated><content type="html">Part 3 - Example store integration architecture  In  from this series shared a look at the logical common architectural elements found in supply chain integration for retail stores. The process was laid out how I've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  It started with laying out the process of how I've approached the use case by researching successful customer portfolio solutions as the basis for a generic architectural blueprint. Having completed our discussions on the logical view of the blueprint, it's now time to look at a specific example. This article walks you through an example store integration scenario showing how expanding the previously discussed elements provides a blueprint for your own store integration scenarios. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's my intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but I've chosen a format that I hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution. STORE INTEGRATION ARCHITECTURE The story behind store integration of their supply chain is that you have many distinct actors involved, as you see starting on the right side of the diagram above. There are suppliers, order managers, warehouse management staff, and any number of third-party supply chain systems that need to have access for providing data to and from the supply chain systems.  Using API management regulates the authorisation and authentication before access is granted to the integration framework. Note the use of event streams which is an indicator that there is an attempt to provide quick processing and action around events in the supply chain lifecycle. This allows for up to the minute reporting on data, supply volumes, and other interesting data for the retail organisation. Message transformation is a core need when connecting systems and data messages through an integration backend, as not all messages are going to map easily from one system to another, or from one microservice to another. Using a transformation service allows for a consistent data model for the organisation regardless of the destination end point needs.  The core integration framework is found in the supply chain microservices, a collection of microservice integrations that manages all the needs of the actors directly providing or requesting information around the retail supply chain. Supporting access to the Retail Data Framework, a separate and detailed architecture blueprint to be covered in another series, you'll find integration data microservices specifically helping with data access and processing. Along the same lines you have integration microservices tying together all external systems such as third-party supply chain systems and any eventual use of an AI / Machine Learning platform. The core to supply chain integration remains a solid fundamental need for cloud-native integration technologies and ongoing maintenance structures in a retail organisation to support them. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the supply chain integration use case.  An overview of this series on the supply chain integration portfolio architecture blueprint can be found here: 1. 2. 3. Catch up on any articles you missed by following one of the links above. This completes the series and we hope you enjoyed this architecture blueprint for supply chain integration in retail. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VbDHkj9ufWE" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/57bT5V1qM3Y/supply-chain-integration-example-store-integration-architecture.html</feedburner:origLink></entry></feed>
